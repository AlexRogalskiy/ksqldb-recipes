---
seo:
  title: Filtering Confluent logs for Splunk
  description: This recipe demonstrates how to filter Conflunet audit logs to Splunk for SIEM processing
---

# Filter Kafka Audit Logs for output to Splunk

In the Security Information and Event Management (SIEM) world, it's just as important have insight into internal actvities as it is to monitor for external security threats and vulnerabilities.  But viewing all internal audit logs would provide too much information, you need to narrow the scope to particular events.

This recipe demonstrates how to filter [Confluent Cloud](https://www.confluent.io/confluent-cloud/tryfree/?utm_source=github&utm_medium=ksqldb_recipes&utm_campaign=filter_logs_for_splunk) audit logs of your cluster for specific events and forward them Splunk for indexing via the [Splunk Sink connector](https://docs.confluent.io/cloud/current/connectors/cc-splunk-sink.html#cc-splunk-sink).
The stream processing application filters for events involving operations on `topics`, but you can review the [structure of Confluent audit logs](https://docs.confluent.io/platform/current/security/audit-logs/audit-logs-concepts.html#audit-log-content) and extend this solution to filter for any [auditable event](https://docs.confluent.io/platform/current/security/audit-logs/audit-logs-concepts.html#auditable-events), 

## Step-by-step

### Setup your Environment

Provision a Kafka cluster in [Confluent Cloud](https://www.confluent.io/confluent-cloud/tryfree/?utm_source=github&utm_medium=ksqldb_recipes&utm_campaign=filter_logs_for_splunk).

--8<-- "docs/shared/ccloud_setup.md"

### Read the data in

For this recipe you'll create a stream to read in your cluster's audit contained in a topic, then create an additional stream where you will filter out unwanted events and only select the data you need to analyze. 

--8<-- "docs/shared/manual_insert.md"

### Run stream processing app

The first stream you create sets a schema for the audit log data which will enable you to selectively pull out only the parts you need to analize.  We'll discuss this more in the [Explaination section](index.md#explaination)

--8<-- "docs/shared/ksqlb_processing_intro.md"

```sql
--8<-- "docs/real-time-analytics/filter-logs-for-splunk/process.sql"
```

--8<-- "docs/shared/manual_cue.md"

```sql
--8<-- "docs/real-time-analytics/filter-logs-for-splunk/manual.sql"
```

### Write the data out

--8<-- "docs/shared/connect.md"

```json
--8<-- "docs/real-time-analytics/filter-logs-for-splunk/sink.json"
```

### Cleanup

--8<-- "docs/shared/cleanup.md"

## Explaination

The first step of this recipe was to [create a stream](https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/create-stream/) from the audit log topic.  By creating a stream, we can assign a schema or [model the JSON](https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/create-stream/).  By applying a model or schema, you can now directly query nested sections of the data, which allows you to apply filters so you only pull back the log entries you're intereseted in.  Additionally you can cherry-pick selected fields from the nested structure, so you can limit the amount of data you retrieve with the query.  The second part of the recipe leverages the applied schema to specify you only want log entries pertaining to events on Kafka `topics`. 

The [JSON structure of the Confluent log entries](https://docs.confluent.io/platform/current/security/audit-logs/audit-logs-concepts.html#audit-log-content) contain several fields:

```json
{   "id": "889bdcd9-a378-4bfe-8860-180ef8efd208",
    "source": "crn:///kafka=8caBa-0_Tu-2k3rKSxY64Q",
    "specversion": "1.0",
    "type": "io.confluent.kafka.server/authorization",
    "time": "2019-10-24T16:15:48.355Z",  <---Time of the event
    "datacontenttype": "application/json",
    "subject": "crn:///kafka=8caBa-0_Tu-2k3rKSxY64Q/topic=app3-topic",
    "confluentRouting": {
        "route": "confluent-audit-log-events"
    },
    "data": {  <--- Relevant data of the event
      ...
    "authorizationInfo": {
        "granted": true,
        "operation": "Create",
        "resourceType": "Topic",  <--- You only want events involving topics
        "resourceName": "app3-topic",
        "patternType": "LITERAL",
        "superUserAuthorization": true
      }
     ... 
    }
}

```
Of these fields in the JSON structure you're only interested in the `time` of the event and the `data` field which contains the specifics the log event, which in this case is any operations where the `resourceType` is `Topic`.  So the first step is to apply a schema to this JSON:

```sql

log_events STRUCT< 
            id VARCHAR, 
            source VARCHAR, 
            specversion VARCHAR, 
            type VARCHAR, 
            time VARCHAR,  
            datacontenttype VARCHAR, 
            subject VARCHAR, 
            confluentRouting STRUCT<
                route VARCHAR >,  
            data STRUCT<
                serviceName VARCHAR, 
                methodName VARCHAR, 
                resourceName VARCHAR, 
                authenticationInfo STRUCT<
                    principal VARCHAR>, 
                authorizationInfo STRUCT<
                    granted BOOLEAN,
                    operation VARCHAR,
                    resourceType VARCHAR,
                    resourceName VARCHAR,
                    patternType VARCHAR,
                    superUserAuthorization BOOLEAN>
  ...
```  
By supplying a schema to the ksqlDB `STREAM`, you are describing the structure of the data to ksqlDB.  The top-level `log_events` `STRUCT` corresponds to the outermost `{` of the JSON structure and gives a name to the entire JSON entry. The name you supply here is arbitrary but it should be meaningful. You'll notice that there are additional nested `STRUCT` fields representing nested JSON objects within the structure.

Now that you've described the structure of the data (applied a schema), you can now create another `STREAM` that will only contain the data of interest. Let's review the query used to create the `STREAM` in two parts, the `CREATE` and the `SELECT` statements.

```sql
CREATE STREAM audit_log_topics
WITH (
  KAFKA_TOPIC='topic-operations-audit-log', 
  TIMESTAMP='time', 
  TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ss.SSSX',
  PARTITIONS=6
) 
```
This `CREATE STREAM` statement specifies to use (or create if it doesn't exist yet) a Kafka topic to store the results of the stream.  The `TIMESTAMP` field tells ksqlDB the field to use for the timestamp for each entry in the stream and `TIMESTAMP_FORMAT` indicates the format of the timestamp field.

The `SELECT` part of the query is where you can drill down to pull out only the records you have an interest in from the orignal stream, let's take a look at each line:

```sql
SELECT LOG_EVENTS->time as time, LOG_EVENTS->DATA
```
Here you specify that you only want the `time` field and the nested `data` entry from the orignal JSON. In ksqlDB you can access nested JSON objects using the `->` operator.

```sql
FROM  audit_log_events
```
The `FROM` clause simply tells ksqlDB to pull the records from the orginal stream you created to model the Confluent log data.

```sql
WHERE (LOG_EVENTS->DATA->AUTHORIZATIONINFO->RESOURCETYPE = 'Topic') 
```
In this `WHERE` statement you drill down several layers of nested JSON using the `->` operator to specify that the records of the new stream are only those entries involving topic operations.





